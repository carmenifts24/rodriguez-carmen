# -*- coding: utf-8 -*-
"""002_Spacy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wmNCp1IJL0dAJNwjWTWgqdzwP1yCt4_e
"""

#ğŸ§  Estas lÃ­neas instalan herramientas externas:
#watermark: opcional, para ver versiones de librerÃ­as (no se usa en el cÃ³digo).
#spacy: biblioteca principal.
#es_core_news_sm: modelo de lenguaje en espaÃ±ol (versiÃ³n pequeÃ±a).

!pip install watermark -q

!pip install spacy -q

# Commented out IPython magic to ensure Python compatibility.
# %load_ext watermark
# %watermark
# %watermark --iversions

!python -m spacy download es_core_news_sm -q

import spacy

import spacy
from spacy import displacy

nlp = spacy.load("es_core_news_sm")
import es_core_news_sm
nlp = es_core_news_sm.load()
# Importamos spaCy y cargamos el modelo entrenado en espaÃ±ol "es_core_news_sm".
# Este modelo permite hacer tokenizaciÃ³n, lematizaciÃ³n, POS tagging, NER y mÃ¡s.

"""## ğŸŸ© CASO 1: TokenizaciÃ³n, lematizaciÃ³n y POS tagging"""

doc = nlp(u'Yo, MatÃ­as Barreto, estoy volando hacia Buenos Aires.')
print([w.text for w in doc])

#ğŸ” Muestra los tokens (palabras individuales), correctamente segmentados.

# Length of sentence
print("La cantidad de tokens es: ", len(doc))
#ğŸ‘‰ Cuenta cuÃ¡ntos tokens hay en el documento.

# Print individual words (i.e., tokens)
print("Los tokens son: \n" + "_" * 20 + "\n")
for words in doc:
    print(words)
    #ğŸ‘‰ Imprime cada palabra/token como un objeto Token.

for token in doc:
  print(token.text, token.lemma_)
  #ğŸ‘‰ Muestra el lema de cada token (forma base).

# Iterar sobre los tokens en el documento y mostrar el texto y su etiqueta POS
for token in doc:
    print(token.text, token.pos_, spacy.explain(token.pos_))


#ğŸ‘‰ Muestra: La palabra. Su categorÃ­a gramatical (POS): NOUN, VERB, etc. La explicaciÃ³n textual de esa etiqueta
# Aplicamos tokenizaciÃ³n, lematizaciÃ³n y POS tagging sobre el texto.
# Imprimimos cada palabra con su forma base (lema) y su categorÃ­a gramatical.

from collections import Counter

word_freq = Counter(doc)
print(word_freq)
#ğŸ‘‰ Intenta contar la frecuencia de tokens. Pero Counter(doc) cuenta objetos Token, no texto, asÃ­ que no da frecuencias legibles directamente.

#otra opciÃ³n para hacerlo
word_freq = Counter([token.text.lower() for token in doc if not token.is_punct])
print(word_freq)

"""## ğŸŸ¦ CASO 2: MÃ¡s lematizaciÃ³n + dependencias"""

doc = nlp(u'En Septiembre de 2025 estaremos realizando una introducciÃ³n a spaCy para trabajar conceptos basicos de PNL')
for token in doc:
  print(token.text, token.lemma_)

#ğŸ‘‰ Procesa un texto mÃ¡s largo y muestra las formas base (lemas).

from spacy.lang.es.examples import sentences

doc = nlp(sentences[11])
print(doc.text)
for token in doc:
    print(token.text, token.pos_, token.dep_)

#ğŸ” Muestra el texto ejemplo sentences[11] del modelo en espaÃ±ol y para cada palabra:
#pos_: categorÃ­a gramatical
#dep_: relaciÃ³n sintÃ¡ctica (sujeto, verbo raÃ­z, modificador, etc.)

word_freq = Counter(doc)
print(word_freq)#frecuencia de apariciÃ³n de cada palabra

displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})
#ğŸ” Visualiza las relaciones de dependencia sintÃ¡ctica entre palabras.
# Utilizamos displacy para mostrar visualmente la estructura sintÃ¡ctica del texto.

"""## ğŸŸ§ CASO 3: Reconocimiento de entidades nombradas (NER)"""

# from spacy import displacy # Moved to cell FMptUiDxonAy

text = "Queremos viajar desde Buenos Aires a Mar del Plata y unos dias depues a La Plata"
doc = nlp(text)
displacy.render(doc,style='ent',jupyter=True,options={'distance':200})
#ğŸ‘‰ Visualiza las entidades encontradas: Buenos Aires, Mar del Plata, La Plata â†’ ubicaciones

text = "Su nombre es Benjamin; naciÃ³ en Argentina."
doc = nlp(text)
displacy.render(doc,style='ent',jupyter=True,options={'distance':200})
#ğŸ‘‰ Detecta nombres propios (Benjamin) y paÃ­ses (Argentina).

displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})

# Mostramos entidades nombradas (personas, lugares, fechas) reconocidas automÃ¡ticamente por spaCy.
#ğŸ‘‰ Alterna entre visualizaciÃ³n de entidades (ent) y de dependencias (dep).

"""## ğŸŸ¨ CASO 4: Chunking y Noun Phrases"""

for token in nlp("Mis padres y amigos viven en Buenos Aires."):
    print(token.text)

for chunk in nlp("Mis padres y amigos viven en Buenos Aires.").noun_chunks:
      print(chunk.text)

#ğŸ‘‰ Se muestra: Cada token del texto
#Y luego los "noun chunks", es decir, grupos de palabras que funcionan como sustantivos.

"""## ğŸŸ¥ CASO 5: Personalizar el pipeline de spaCy
Detectar que "Baires" es "Buenos Aires"
"""

from spacy.symbols import ORTH, NORM

doc = nlp(u"Estoy volando hacia Baires")
print([w.text for w in doc])

from spacy.language import Language
from spacy.tokens import Token

# Componente personalizado para asignar lemas especÃ­ficos
@Language.component("custom_lemmatizer")
def custom_lemmatizer(doc):
    for token in doc:
        if token.text == "Baires":
            token.lemma_ = "Buenos Aires"
    return doc
    #ğŸ‘‰ Crea un componente personalizado que modifica el lema de la palabra "Baires".

# Remove the existing custom component if it exists
if "custom_lemmatizer" in nlp.pipe_names:
    nlp.remove_pipe("custom_lemmatizer")

# AÃ±adir el componente al pipeline
nlp.add_pipe("custom_lemmatizer", after='lemmatizer')
#ğŸ‘‰ Se agrega al pipeline despuÃ©s del componente de lematizaciÃ³n original.

# Ejemplo de texto en espaÃ±ol
doc = nlp(u'Estoy volando a Baires')
print([w.text for w in doc])
print([w.lemma_ for w in doc])