# -*- coding: utf-8 -*-
"""002_Spacy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wmNCp1IJL0dAJNwjWTWgqdzwP1yCt4_e
"""

#🧠 Estas líneas instalan herramientas externas:
#watermark: opcional, para ver versiones de librerías (no se usa en el código).
#spacy: biblioteca principal.
#es_core_news_sm: modelo de lenguaje en español (versión pequeña).

!pip install watermark -q

!pip install spacy -q

# Commented out IPython magic to ensure Python compatibility.
# %load_ext watermark
# %watermark
# %watermark --iversions

!python -m spacy download es_core_news_sm -q

import spacy

import spacy
from spacy import displacy

nlp = spacy.load("es_core_news_sm")
import es_core_news_sm
nlp = es_core_news_sm.load()
# Importamos spaCy y cargamos el modelo entrenado en español "es_core_news_sm".
# Este modelo permite hacer tokenización, lematización, POS tagging, NER y más.

"""## 🟩 CASO 1: Tokenización, lematización y POS tagging"""

doc = nlp(u'Yo, Matías Barreto, estoy volando hacia Buenos Aires.')
print([w.text for w in doc])

#🔍 Muestra los tokens (palabras individuales), correctamente segmentados.

# Length of sentence
print("La cantidad de tokens es: ", len(doc))
#👉 Cuenta cuántos tokens hay en el documento.

# Print individual words (i.e., tokens)
print("Los tokens son: \n" + "_" * 20 + "\n")
for words in doc:
    print(words)
    #👉 Imprime cada palabra/token como un objeto Token.

for token in doc:
  print(token.text, token.lemma_)
  #👉 Muestra el lema de cada token (forma base).

# Iterar sobre los tokens en el documento y mostrar el texto y su etiqueta POS
for token in doc:
    print(token.text, token.pos_, spacy.explain(token.pos_))


#👉 Muestra: La palabra. Su categoría gramatical (POS): NOUN, VERB, etc. La explicación textual de esa etiqueta
# Aplicamos tokenización, lematización y POS tagging sobre el texto.
# Imprimimos cada palabra con su forma base (lema) y su categoría gramatical.

from collections import Counter

word_freq = Counter(doc)
print(word_freq)
#👉 Intenta contar la frecuencia de tokens. Pero Counter(doc) cuenta objetos Token, no texto, así que no da frecuencias legibles directamente.

#otra opción para hacerlo
word_freq = Counter([token.text.lower() for token in doc if not token.is_punct])
print(word_freq)

"""## 🟦 CASO 2: Más lematización + dependencias"""

doc = nlp(u'En Septiembre de 2025 estaremos realizando una introducción a spaCy para trabajar conceptos basicos de PNL')
for token in doc:
  print(token.text, token.lemma_)

#👉 Procesa un texto más largo y muestra las formas base (lemas).

from spacy.lang.es.examples import sentences

doc = nlp(sentences[11])
print(doc.text)
for token in doc:
    print(token.text, token.pos_, token.dep_)

#🔍 Muestra el texto ejemplo sentences[11] del modelo en español y para cada palabra:
#pos_: categoría gramatical
#dep_: relación sintáctica (sujeto, verbo raíz, modificador, etc.)

word_freq = Counter(doc)
print(word_freq)#frecuencia de aparición de cada palabra

displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})
#🔍 Visualiza las relaciones de dependencia sintáctica entre palabras.
# Utilizamos displacy para mostrar visualmente la estructura sintáctica del texto.

"""## 🟧 CASO 3: Reconocimiento de entidades nombradas (NER)"""

# from spacy import displacy # Moved to cell FMptUiDxonAy

text = "Queremos viajar desde Buenos Aires a Mar del Plata y unos dias depues a La Plata"
doc = nlp(text)
displacy.render(doc,style='ent',jupyter=True,options={'distance':200})
#👉 Visualiza las entidades encontradas: Buenos Aires, Mar del Plata, La Plata → ubicaciones

text = "Su nombre es Benjamin; nació en Argentina."
doc = nlp(text)
displacy.render(doc,style='ent',jupyter=True,options={'distance':200})
#👉 Detecta nombres propios (Benjamin) y países (Argentina).

displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})

# Mostramos entidades nombradas (personas, lugares, fechas) reconocidas automáticamente por spaCy.
#👉 Alterna entre visualización de entidades (ent) y de dependencias (dep).

"""## 🟨 CASO 4: Chunking y Noun Phrases"""

for token in nlp("Mis padres y amigos viven en Buenos Aires."):
    print(token.text)

for chunk in nlp("Mis padres y amigos viven en Buenos Aires.").noun_chunks:
      print(chunk.text)

#👉 Se muestra: Cada token del texto
#Y luego los "noun chunks", es decir, grupos de palabras que funcionan como sustantivos.

"""## 🟥 CASO 5: Personalizar el pipeline de spaCy
Detectar que "Baires" es "Buenos Aires"
"""

from spacy.symbols import ORTH, NORM

doc = nlp(u"Estoy volando hacia Baires")
print([w.text for w in doc])

from spacy.language import Language
from spacy.tokens import Token

# Componente personalizado para asignar lemas específicos
@Language.component("custom_lemmatizer")
def custom_lemmatizer(doc):
    for token in doc:
        if token.text == "Baires":
            token.lemma_ = "Buenos Aires"
    return doc
    #👉 Crea un componente personalizado que modifica el lema de la palabra "Baires".

# Remove the existing custom component if it exists
if "custom_lemmatizer" in nlp.pipe_names:
    nlp.remove_pipe("custom_lemmatizer")

# Añadir el componente al pipeline
nlp.add_pipe("custom_lemmatizer", after='lemmatizer')
#👉 Se agrega al pipeline después del componente de lematización original.

# Ejemplo de texto en español
doc = nlp(u'Estoy volando a Baires')
print([w.text for w in doc])
print([w.lemma_ for w in doc])